# Auto-booking Backend Architecture

Parker Flight Auto‑Booking Backend Architecture
High-level architecture for Parker Flight’s auto‑booking system, showing how campaigns are created, monitored, and automatically converted into bookings.
System Overview and Components
Parker Flight’s auto-booking backend consists of three major parts: campaign management, deal monitoring, and booking execution. The system builds on existing infrastructure – a robust booking state machine (with Saga rollback, idempotency locks, and error handling) and a comprehensive database schema – and fills in the remaining pieces for end-to-end automation. At a high level, the flow is: a user creates an auto-booking campaign (with trip criteria, traveler info, and payment on file); a background job periodically searches for matching flight deals; when a qualifying deal is found, the system triggers the booking state machine to charge the user and book the flight via Duffel’s API. The database is updated and the user is notified of the successful booking or any failures. All components communicate through the Supabase backend (Edge Functions and Postgres DB), ensuring consistency and leveraging the existing tables and services. Key components and their roles:
Campaign API (Edge Functions): Secure endpoints for creating, updating, listing, and deleting auto-booking campaigns. These write to the auto_booking_requests (campaigns) table and associated tables (traveler profiles, payment methods, etc.).
Scheduler & Campaign Processor: A scheduled job (using Supabase’s pg_cron) that wakes up periodically to scan for active campaigns that need checking. For each active campaign, it performs a flight search (via Amadeus or Duffel offers API) and evaluates results against the campaign’s criteria (deal detection).
Booking State Machine (Edge Function): The existing auto-book/index.ts function implements a multi-step booking saga (search, price verify, charge payment, book ticket, etc.) with full rollback and idempotency. This is invoked when the Campaign Processor finds a valid deal, to carry out the actual charge and booking.
External APIs: The system integrates with Stripe for payments and Duffel for flight booking. Stripe handles off-session payment capture (the card is charged automatically without user present), and Duffel provides flight offers, order creation, and ticketing. We retain Amadeus integration for flight search or as a fallback source if needed.
Notifications: The existing notification system (emails/push) is used to inform users of important events – e.g. when a booking is confirmed, when a campaign can’t be fulfilled, or if a payment fails. This keeps users in the loop at every critical point in the campaign lifecycle.
This architecture is designed to be scalable (able to handle thousands of concurrent campaigns by batching and scheduling work), reliable (using idempotent operations and state tracking to avoid duplicate bookings or charges), and secure (no sensitive data stored in plaintext, all payments via Stripe, and PII encrypted in the database). Next, we detail each aspect of the system and how it will be implemented.
Campaign Lifecycle Management
Campaign Definition: A “campaign” represents a user’s long-running flight search criteria plus the instructions to auto-book if conditions are met. Users define campaigns via a Create Campaign API call, providing details such as origin, destination (or region), travel dates or range, number of travelers, cabin class, and a maximum price (budget) they’re willing to pay. Each campaign is tied to a specific traveler profile and a saved payment method. The backend validates the input (e.g. dates in the future, budget is a sensible amount) and stores the campaign in the database, e.g. in an auto_booking_requests table (or a similar campaigns table) with status active. Key fields include the user ID, traveler_profile_id, payment_method_id, search criteria (destinations, date range, etc.), price threshold, and campaign status/state. Campaign States: We implement a campaign state machine to manage the lifecycle of each campaign. States include: active (monitoring for deals), booked (a booking was made and campaign is fulfilled), paused (temporarily not checking, e.g. due to payment issues or user request), completed (ended after a booking or end-date), expired (ended with no booking by a certain date), and cancelled (user manually stopped the campaign). Transitions happen as follows:
Active → Booked/Completed: when an auto-booking is successfully executed, the campaign is marked completed (no further searches).
Active → Expired: if the campaign’s time window passes (e.g. latest departure date) without a successful booking, it transitions to expired.
Active ↔ Paused: the user can pause a campaign (or the system might pause it automatically on certain errors like payment failure). A paused campaign is skipped by the processor until resumed. Users may resume to continue searching.
Active/Paused → Cancelled: user can cancel if they no longer want to continue. Cancelled campaigns are not processed further.
Any → Failed: (optional) if an unrecoverable error occurs (like consistent API failures), we might mark a campaign as failed, but generally we’ll use paused or notify the user rather than a distinct failed state.
The CampaignService (in src/services/autobooking/CampaignService.ts) encapsulates business logic for these state transitions and validations. For example, on creation it sets initial status and schedules the first search, on pause it updates status and perhaps cancels any scheduled jobs, and on completion it ensures all related resources are finalized (e.g. prevents further charges). The state machine ensures only valid transitions (e.g. you can’t resume a completed campaign, etc.). Campaign CRUD API: We will implement dedicated RESTful endpoints (as Supabase Edge Functions) for managing campaigns:
POST /campaigns – Create a new campaign. This will parse the user’s JSON input, call the CampaignService to validate and insert a new record, and respond with the created campaign details. On creation, we may trigger an immediate initial search or set the next check time.
GET /campaigns – List the user’s campaigns (active, paused, etc.), so they can see status in the dashboard.
GET /campaigns/:id – (if needed) retrieve details of a specific campaign, including current status, last search time, etc.
PUT /campaigns/:id – Update a campaign’s criteria or status. We allow certain edits like changing the price limit or pausing/resuming the campaign. Some fields (like origin/destination) might not be editable after creation (to avoid confusion), but price or date range could be adjusted. This endpoint will enforce allowed changes based on state (e.g. you can’t modify an already-booked campaign).
DELETE /campaigns/:id – Cancel a campaign. This sets status to cancelled and removes it from active monitoring. (We might keep the record for history, but mark it inactive).
All these endpoints will authenticate the user (using Supabase Auth JWT) and apply row-level security so users only modify their own campaigns. The functions correspond to files like create-campaign.ts, update-campaign.ts, etc., as noted in the project structure. Each API call only performs quick DB operations (and possibly scheduling), deferring heavy work (like running a search) to the background processor. Traveler Profiles and Payment: During campaign creation, the system ensures we have a traveler profile and a payment method on file. If not, the UI will prompt the user to create/save these first (this flow is largely implemented per the existing payment architecture design). The campaign stores references to a specific traveler and payment method. This allows one user to manage multiple travelers or payment options in the future. For now, the MVP can assume one traveler = the user, and one default payment, but the schema is built to support one-to-many relationships (one user to many traveler_profiles, etc.). Traveler data (name, DOB, passport, etc.) is stored encrypted and fetched only when needed for booking. Campaign Criteria & Deal Definition: Each campaign encapsulates the search parameters. The simplest “deal detection” logic is to book any flight that meets the user’s max price and other criteria. For MVP, if the fare is equal or below the user’s budget, it’s considered a deal. The campaign might also allow filters like maximum stops, preferred airlines, or specific travel dates vs. flexible range – the deal detection will honor those. In future, more complex algorithms (e.g. notifying the user but not booking if a flight is slightly above budget but very good value, or using historical price data to judge a “good deal”) can be layered on. Initially, the user explicitly sets the threshold, which keeps the logic straightforward: find cheapest available fare; if price <= budget, trigger booking. This rule is evaluated whenever new search results are obtained. Persistence and Duration: Campaigns can run for months until they find a deal or expire. We store a created_at and optionally an expires_at or “travel_by” date. The system will keep checking until that date. To handle long durations, we maintain a last_searched_at timestamp to avoid redundant searches too frequently. We also store interim state like next_search_at (which the scheduler can use to decide which campaigns to process). The campaign record might also keep track of how many searches have been performed, or any near-miss deals (for analytics). All data persists in Postgres; if the system restarts, it can resume where it left off because campaign state is in the DB. In summary, the campaign management architecture provides a self-serve interface for users to configure their desired trip and then entrust the system to continuously look for them. The backend’s job is to honor those settings, keep the user’s data secure, and update the campaign state as it moves through its lifecycle (searching, booked, etc.). Next, we describe how the continuous monitoring works in detail.
Background Job for Deal Monitoring
Once a campaign is active, a background scheduler takes over the responsibility of monitoring for deals. Supabase’s hosted platform supports cron jobs in the database (via the pg_cron extension) to invoke functions on a schedule
supabase.com
. We leverage this to run a Campaign Processor function at a regular interval. A sensible default frequency is hourly checks, but this can be tuned: for example, run every hour for trips departing soon, or every few hours for trips far in the future to reduce unnecessary API calls. We might even use a dynamic schedule per campaign (set next_search_at for each and have the scheduler run more often but skip campaigns not due yet). Initially, an hourly sweep of all active campaigns is simple and effective. Campaign Processor Workflow: The scheduled function (e.g. supabase/functions/campaigns/campaign-processor.ts) executes roughly the following steps:
Fetch Due Campaigns: Query the auto_booking_requests (campaigns) table for all campaigns with status active (and not paused) that are due for a search. This could be all active campaigns if we search at a fixed frequency, or those where NOW() >= next_search_at. We also ensure the current time is within any user-defined search window (if they only want searches during certain hours, though that’s an edge case).
Loop Through Campaigns: For each campaign found, attempt to find a flight deal:
Search for Flights: Use the flight search API. We have two possible approaches:
Amadeus API: If the existing system uses Amadeus for flight search (as indicated by current state machine integration), we can call Amadeus with the campaign’s criteria (origin, destination, date or date range, cabin, travelers) to get available fares.
Duffel Offers API: Alternatively, use Duffel’s offer request API to search flights (Duffel can search and return offers which are directly bookable). A hybrid approach is also possible: use Amadeus to identify deals, then use Duffel to actually book them. For reliability and to reduce dependency on multiple providers, using Duffel for both search and booking is a clean approach (one less integration point). We can create an offer request via Duffel each time: POST /air/offer_requests with the route, dates, and traveler info, then GET /air/offers to retrieve offers.
Filter & Detect Deal: Once results are obtained (a list of flight offers with prices and details), apply the campaign’s criteria:
Filter out anything not matching essential criteria (e.g. if user set max 1 stop and an offer has 2 stops, exclude it).
Among the valid offers, find those that meet the price threshold (offer total <= campaign.budget). If none are cheap enough, this campaign has no deal this round.
If at least one qualifies, identify the best offer to book. “Best” could be lowest price or a combination of factors (e.g. consider shortest travel time if prices are similar). For MVP, simply pick the cheapest offer under the threshold.
Schedule Next Check or Trigger Booking:
If no qualifying offer is found, update the campaign’s last_searched_at and compute a next_search_at. The next search could be simply NOW() + 1 hour for uniform frequency, or a slightly longer interval if needed. This ensures we don’t re-check too quickly when nothing has changed. The campaign remains active and will be picked up in the next cycle.
If a deal is found, the Campaign Processor will initiate the booking process for that campaign immediately (before the fare expires or changes). This is done by calling the booking state machine function (described below) and passing along the necessary data (campaign id or offer details). The campaign’s status might be updated to something like processing or booking_attempt while we attempt the booking (to prevent duplicate processing by another job or thread). We also record the offer details (offer ID, price) in a booking_requests table or similar, to have a log of what we attempted to book.
Concurrency & Parallelism: To handle many campaigns, the processor can operate in parallel. Supabase Edge Functions can make outbound calls to itself or others; one strategy is that the scheduler function identifies campaigns and then spawns a separate function invocation for each campaign (e.g. making an HTTP call to process-booking function with the campaign ID). This way, multiple bookings can be processed concurrently, and the scheduler isn’t blocked on one campaign. The design in our project hints at this separation: a scheduler-booking function versus a process-booking function. We will use that pattern: the cron-triggered function enqueues or invokes the per-campaign processor. Each campaign processing run will independently search and possibly book. This isolation also improves fault tolerance (one campaign failing doesn’t crash the whole loop). We need to guard against the same campaign being processed twice in parallel – using a DB lock or status flag can ensure that (our booking_attempts table and saga lock is already in place for the booking step).
Retry and Backoff: If an API call to search fails (e.g. Duffel search API times out), the processor should handle it gracefully. We can implement a simple retry with exponential backoff for transient errors (e.g. try again in a few seconds up to a couple of attempts). If the search consistently fails or the API is down, we log the error and leave the campaign active for the next cycle. (If a critical outage occurs, see fallback strategy below.)
Monitoring Frequency: The default frequency is hourly, which balances finding deals quickly with not overloading the API or hitting rate limits. We can refine this by campaign urgency:
For campaigns with travel dates soon (next few weeks), check more often (hourly or even 30 minutes if needed during final days when prices can drop rapidly).
For campaigns far in the future, checking a few times a day might suffice initially.
We also consider that airlines load fares at certain times of day and fare sales appear unpredictably; an hourly check ensures we catch most changes without too much delay. The system could also incorporate event-driven triggers (e.g. if we had a price alert feed) in future, but those are not assumed available. The pg_cron scheduler is reliable and will run 24/7, so even overnight or months later it will continue scanning as long as the campaign remains active.
Deal Detection Logic: As described, our primary deal criterion is price <= user’s max price. We could augment this with additional logic: for example, if the user’s date range is flexible, the search might find a flight on a slightly different date that’s cheaper. We should clarify how flexible date searches are handled:
If user specified a fixed date, we search that date.
If a range (say “anytime in June”), we may perform multiple searches or use an API that supports range (Duffel’s API might allow a range search, or we loop through dates).
The Campaign Processor can iterate through a date range (e.g. check each day of a week, or use a ±N days flexibility) in each cycle. However, to keep things efficient, we might break it up or only search a few days per run and cover the whole range over time.
For MVP, we might limit to exact dates or a small date window to search each time. Advanced logic can include tracking the lowest fare seen so far and only notifying/booking if a new low appears, but since the user provides an explicit threshold, we stick to that for auto-booking trigger.
Recording Search Results: For transparency and not to repeat work, we may store some of the search outcomes. For example, we could have a campaign_search_logs table logging when we searched and what the best price found was. This can be used to inform users (e.g. “current lowest price $450, your target $400”) and also to optimize (if we saw $410 last time, maybe it’s close to threshold – perhaps nudge the user or adjust strategy). This is a nice-to-have and can be phase 3. It’s not required for functionality, but it supports performance (by caching results) and user engagement. In summary, the background job system acts as the heartbeat of the auto-booking feature, continuously scanning for opportunities and dispatching booking tasks when criteria are met. It is designed to be efficient (processing only due campaigns, filtering results quickly) and robust (using retries and isolating failures). Now, once a deal is identified, the next step is handing off to the booking engine – which we cover next.
Duffel API Integration Strategy
Integrating Duffel is critical for turning a flight offer into a confirmed ticket. Our strategy is to use Duffel for the booking execution while leveraging its capabilities (offer booking, payments, ticket issuance). Key aspects include handling offer expirations, order creation with payment, and webhook events from Duffel for post-booking updates. Offer Search vs. Booking: We have two possible entry points:
Full Duffel Workflow: Use Duffel from search to booking. That is, the Campaign Processor creates an Offer Request via Duffel and retrieves offers. When it finds a suitable offer, it will use that Duffel offer directly for booking. This has the advantage that the offer is guaranteed bookable through Duffel (no need to cross-reference Amadeus data).
Hybrid Workflow: Use Amadeus for searching deals (perhaps leveraging existing code or broader search capabilities), then when a deal is found, recreate or verify it on Duffel. In this case, given an Amadeus result (with flight segments and price), we would call Duffel’s API to price that itinerary. This can be done by searching Duffel for the same route/date to get a matching offer, or using Duffel’s offer request with specific parameters. If Duffel finds that exact flight, we proceed to booking; if not, the system may fall back or not book. This approach is more complex and riskier because it involves synchronizing two systems’ data.
Given Duffel’s API can handle searches and is needed for booking, the simplest approach is to use Duffel for search and booking in one flow. This ensures consistency (the offer we book is the one we found) and speeds up booking. We will implement a DuffelService (or extend the existing DuffelClient) with methods to cover:
searchOffers(request: OfferRequestParams): Combines creating an offer request and fetching offers.
createOrder(offerId, travelerInfo, payment): Creates a booking (order) on Duffel for a given offer.
confirmPayment(orderId): If using Duffel Payments, confirm payment (but in our case we likely charge via Stripe externally).
Additional utility: isOfferExpired(offer): checks if the current time is within the offer’s expiration window.
Offer Expiration Handling: Duffel flight offers typically expire in 5 to 20 minutes from creation. This means once we fetch offers, we have a short window to complete the booking. Our architecture handles this by tightly coupling deal detection, payment, and order creation in one flow:
As soon as a qualifying offer is identified by the Campaign Processor, it immediately invokes the booking function. We do not wait or queue for later – it’s done in real-time.
Before booking, we double-check the offer’s expiration timestamp. If the offer is too close to expiring (say less than 2 minutes remaining), we might preemptively refresh or re-search to get a fresh offer. This can be done by calling Duffel’s GET offer by ID to see if it’s still valid, or simply including a small buffer (e.g. if >3 minutes have passed since search, do a new search).
The booking state machine will call Duffel’s order creation immediately after payment authorization. The entire process from finding the offer to completing booking is designed to occur well within that expiry window (usually just a few seconds, dominated by payment processing time). Using Stripe off-session payment is very fast (a couple of seconds at most to confirm) so we expect to be well within 5 minutes.
If by chance the offer expires just as we attempt to book, Duffel will return an error (e.g. offer_no_longer_available). Our error handling will catch this and treat it as a failed booking attempt. In this case, we rollback: if payment was captured, immediately refund it, and notify the user that the deal was missed. The campaign could either continue (stay active to try again next time) or be marked completed/expired. We will likely leave it active (since no booking happened and the user might still want a deal later). However, to avoid rapid re-trigger loop, we might wait until the next cycle to search again.
Order Creation & Payment: We will use Duffel’s Orders API to book the flight:
Prepare Order Data: Gather passenger info (from Traveler Profile), selected offer_id, and any ancillary options (like seating if required, though initial version likely skips seat selection unless Duffel supports specifying or we do afterwards).
Ensure Payment is ready: Parker Flight has two ways to pay the airline via Duffel:
Duffel Pay: letting Duffel charge our saved card or balance for the cost. We might not want to use Duffel’s PCI feature since we already charge the user via Stripe. The likely scenario is Parker maintains a Duffel prepaid balance or uses Duffel’s payments with our own card.
External Payment: Use Stripe to charge the user, then use those funds to pay Duffel. Practically, if using Duffel’s “Pay with Card” option, we could supply the user’s card token to Duffel. But since we prefer to not pass user card around, Parker might use its company card or Duffel balance for the airline payment and treat the user’s Stripe charge as reimbursement. Initially, using Duffel Balance is a safe approach – Parker preloads funds into Duffel’s account and Duffel draws from it when we create an order. Our system ensures Parker’s balance is >= offer cost before proceeding (to avoid order failure).
Two-phase commit (Payment then Booking): Our booking state machine will perform the payment charge first via Stripe, then create the order in Duffel. This sequence is chosen because we don’t want to create a ticket without securing payment from the user. If payment succeeds but order fails, we refund as mentioned. An alternative approach is to create the Duffel order without immediate payment (Duffel might allow holding an order for a brief time before ticketing, or marking it as awaiting payment). If such an API exists, we could initiate order, then confirm payment second. However, to keep it simple and due to the short expiry, we take the straightforward route: charge user -> book order.
Idempotency: We will use idempotency keys for the Duffel order creation call. A suitable key might be the campaign ID or the combination of campaign + offer ID + attempt number. This prevents creating duplicate orders if our function retries or is invoked twice. Similarly, the booking_attempts table in our DB ensures only one booking is processed at a time per campaign.
Webhook Integration: We will register a webhook endpoint to listen to Duffel events like order.created, order.payment_succeeded, order.ticketed. These events can serve as an additional confirmation and for post-booking actions:
order.created: confirms an order was created in Duffel. We can use this to update our bookings table immediately with Duffel’s order ID and status.
order.payment_succeeded: if using Duffel’s payment system, this indicates the airline payment went through. In our case, if we use Duffel balance, this might not fire (since payment is instant from balance). If we ever use Duffel’s card payment flow, this is important.
order.ticketed: this event tells us that e-tickets have been issued for the booking (which can sometimes happen a few seconds or minutes after order creation). Once ticketed, the booking is confirmed. We should update our booking record to mark it ticketed and store ticket numbers if provided.
order.cancelled: if for some reason the order is canceled (maybe by an admin or airline), we handle it (notify user and possibly set campaign back to active if within window).
Our Duffel webhook handler (another Edge Function) will verify the webhook signature and update relevant records, similarly to how we handle Stripe webhooks. This adds resiliency: even if our booking function missed a step, the webhook will ensure we catch the final status.
Fallback to Amadeus or Others: We implement fallback strategies for robustness:
Search Fallback: If Duffel’s search API fails or is down, we can fall back to using Amadeus API to find deals. In this case, we might at least detect a deal and notify the user instead of auto-booking (since booking via Amadeus is not straightforward without a full integration or user involvement). We would likely pause or defer the actual booking until Duffel is available, or as a last resort, send the user the deal details to book manually. Given auto-booking is the promise, this is a contingency plan for outages.
Booking Fallback: If Duffel returns an error at order creation (other than expiration), we could attempt one more route. For example, if Duffel cannot book a particular low-cost carrier (maybe not supported), and Amadeus has a direct booking capability (like through a partner or by holding a reservation), we might attempt that. However, this is very complex and not in MVP scope. Instead, we’ll handle such an error by notifying the user that we couldn’t auto-book and perhaps provide instructions/offers to book manually. In future, with user consent, we could use an alternate provider to book if Duffel fails (ensuring the user’s card is charged only once).
Payment Fallback: We already use Stripe for payments, so we aren’t relying on Duffel’s payment system (which avoids needing to handle 3DS in Duffel, etc.). If Stripe fails (network issue), we might attempt Duffel’s payment as a fallback if that was easier, but since Stripe is our source of truth for charging users, we stick with it. Instead, if Stripe is temporarily down, we simply wait and try later or ask the user for next steps (see Error Handling below).
Duffel Outage Plan: In an extended Duffel outage, the campaign processor might skip or pause campaigns. We will communicate to users if needed (e.g., “Booking service is temporarily unavailable, your campaign is on hold”). This is a rare scenario, but our design (storing campaigns and not depending on in-memory state) means we can resume when Duffel returns.
Seat Selection and Extras: The current state machine had integration for seat selection and possibly baggage. Duffel’s API does allow adding bags or selecting ancillary services at booking time. For MVP, we may skip extras or use defaults (no extra bags unless the fare includes, and auto-select seats or leave unassigned). We should ensure our Duffel integration captures whether the fare includes baggage, etc., to avoid surprises for the user. These details would be communicated in the booking confirmation. In essence, the Duffel integration is implemented as a thin service layer that our booking function uses. It carefully manages the short offer life cycle (ensuring booking is done quickly or not at all if expired), uses idempotent calls to avoid duplicates, and maps Duffel’s responses to our system (storing order IDs, ticket info in our bookings table, etc.). By doing so, we leverage Duffel’s strength (a single API to book across many airlines) within our automated workflow, while safeguarding against its pitfalls (like short expirations and rate limits).
Campaign-to-Booking State Machine Integration
The integration between the campaign monitoring system and the booking state machine is where everything comes together. We design this interface to be event-driven and idempotent, ensuring that a given campaign can trigger a booking at most once unless configured otherwise. Trigger Conditions: A campaign triggers the auto-book sequence when the Campaign Processor finds a qualifying flight offer and the campaign is in a state ready to book (active, not already booking or booked). The logic to decide a trigger is essentially: if (price <= budget AND campaign.status == active AND not already booked) then initiate booking. We also ensure no other overlapping condition like the user being at their booking limit (if, say, we only allow one auto-book per user at a time – but by default users can run multiple campaigns). Data Handoff: When triggering the booking, the Campaign Processor will pass the necessary data to the booking function. This can be done in two ways:
Direct function call with payload: Since we are in Supabase Edge Functions, one function can call another via an HTTP request. The processor can POST to the auto-book function’s endpoint with a payload containing the campaign_id or the specific offer details (offer_id, etc.). Passing just an ID is more lightweight; the booking function can then load all needed data from the DB.
Database-driven trigger: Alternatively, the processor could insert a record into a booking_requests table with campaign_id and offer_id, and the auto-book function could be set to listen (if we had a trigger or another cron) for new booking requests. This is less direct, but could be used for queueing. However, given the time sensitivity, a direct call is preferred.
We will implement it as a direct call: the Campaign Processor makes an HTTP POST to the auto-book function (which is essentially our booking state machine entry point). We include an authorization (e.g. service role token) so that the auto-book function trusts the call (only our system can trigger it, not arbitrary public). The payload includes identifiers, and the auto-book function will:
Lock the campaign (e.g., insert into booking_attempts to obtain a lock as it already does).
Re-verify that the campaign is still valid to book (no race conditions). For example, if two offers triggered very close or a duplicate call happened, the booking function can see if campaign.status is still active and no booking exists yet for it. If it finds a conflict (booking already in progress or done), it should abort to avoid duplicate booking.
Fetch the traveler profile, payment info, and either the flight offer details or re-run a price confirmation step. Price confirmation is important: even though we have an offer, prices can change or we might want to ensure consistency. The booking state machine might call Duffel’s offer-get or reprice endpoint to double-check the fare total just before charging (this can be a last-minute sanity check).
Proceed with payment and booking as per the saga: charge card, create order, etc. (Those steps have been covered above in Duffel integration and payment flow.)
Concurrency for Same User: The system allows a single user to have multiple campaigns (e.g., one for “Hawaii in summer” and another for “Paris in fall”). It’s possible that two campaigns trigger bookings around the same time. We handle this as follows:
Each campaign is processed independently and has its own lock and booking attempt record. So technically two bookings can proceed in parallel for one user.
Stripe considerations: Charging the same user’s card twice simultaneously is generally fine, as long as credit limit covers it. There’s a small chance the bank might flag back-to-back charges as suspicious, but that’s more of a business concern (we could mitigate by using different idempotency keys or letting one finish then the other if needed). At architecture level, we don’t strictly serialize them – we assume if the user set up two campaigns, they are willing to pay for both if they hit.
However, from a user experience perspective, if the user did not intend to actually book two trips (maybe they set two possibilities but only wanted one to book), that’s a product policy issue. We might warn users “if you have multiple campaigns, they might all book if deals appear – ensure you want all of them.” We could consider a feature like “stop other campaigns after one booking succeeds” but that’s not in scope initially.
We will ensure that our booking state machine transactions don’t interfere with each other. They already use transaction boundaries and separate records for each booking. The bookings table would have a unique ID per booking, and booking_attempts ensures one process per campaign. The only shared resource might be the Stripe customer if using the same card: Stripe can handle concurrent intents, but we use strong idempotency keys to tie each PaymentIntent to a specific campaign to avoid any mix-up.
If by some chance two campaigns find the exact same flight (maybe the user duplicated criteria), they could try to book the same flight twice for the same person. That likely would be caught by the airline (duplicate passenger on same flight) or simply result in two separate tickets. To avoid nonsense, we could detect if two active campaigns for the same traveler have overlapping dates and prevent that scenario or at least not double-book the identical trip. This can be a validation: e.g., one user shouldn’t have two active campaigns for the same date and traveler – unless they intend for two seats, but then they should use one campaign with 2 passengers. We will put guidelines in UI and maybe a check in backend to prevent duplicate-trip campaigns.
Synchronization and State Updates: After the booking function runs, it will update relevant tables:
Insert into bookings table the confirmed booking details (PNR, ticket numbers, flight details, etc.).
Update auto_booking_requests (campaign) status to booked or completed and link it to the booking record.
Update any trip_requests or booking_requests tables if those track the attempt. (It sounds like trip_requests might store the original search query, and booking_requests might store attempts; we’ll integrate with whatever scheme exists – from the status, it seems these tables are present and used by the state machine.)
The booking state machine already has logic for database updates (like marking trip_request booked, etc.). We will extend it to also mark the campaign record.
The Campaign Processor, after invoking the booking, can either wait for a response or (if it triggered asynchronously) it might just end its job. Since the booking function is an HTTP call, we can call it synchronously and get a result (success or failure). If we do that, the Campaign Processor can handle the outcome:
On success: mark campaign completed (the booking function might have done it, but double-check) and maybe immediately send a notification to the user that campaign ended with success.
On failure: if the booking couldn’t be completed (payment failed, offer gone, etc.), decide what to do. Likely, we set campaign status back to active but possibly paused if the failure reason is something that needs user intervention (e.g. payment failed -> pause campaign until fixed). If failure was due to price change or seat sold (i.e., not user’s fault), we can keep it active to try again next time. We will also log this attempt in booking_requests or similar with a status (failed) for audit.
Idempotency & Duplicate Avoidance: We rely on our booking lock (booking_attempts table) to avoid duplicates. When the auto-book function starts, it inserts a record with campaign_id and a unique attempt id – if a record already exists for that campaign in “in-progress” state, it will abort. This prevents two processes (maybe two overlapping scheduler calls) from booking the same campaign. Additionally, Stripe and Duffel calls all use idempotency keys as noted, to avoid double-charging or double-booking if the function is retried. Our database also enforces unique constraints where appropriate (e.g., one campaign_id per booking in bookings table). Cleaning Up and Continuing: After a successful booking, the campaign is essentially done (one campaign corresponds to one booking in our model). We will not reuse the campaign for multiple bookings (the user would create separate campaigns for separate trips). Thus we mark it completed. If the user wants to run it again (say the trip failed or they want another similar trip), they’d create a new campaign or we could allow “reset” but that’s unnecessary complexity. If a campaign was not successful (e.g. expired), we mark it accordingly and it ends. Users can always start a new one or extend it. In summary, the interface between campaign monitoring and booking execution is a controlled handoff: the monitoring finds an opportunity, calls the booking engine, and then updates the campaign based on the booking result. This decoupling (search vs. booking) follows the single-responsibility principle and makes it easier to test each part. It also means improvements or fixes in the booking state machine benefit both manual and auto booking flows, since they share the same logic for actual ticket issuance and payment.
API Design for Campaign Management
Designing clear API endpoints for campaign management ensures the feature can be easily integrated into the front-end. All campaign-related APIs will be authenticated endpoints (requiring a valid user token) and will operate on the user’s own campaigns. 1. POST /campaigns – Create Campaign:
Request: JSON body with campaign details – e.g. { origin, destination, date_start, date_end, max_price, traveler_profile_id, payment_method_id, cabin_class, ... }. Minimal required fields might be origin, destination, at least one date or range, and max_price.
Behavior: The function will:
Verify the user is authenticated (Supabase does this via JWT).
Validate the input (e.g., max_price is a number > 0, dates are in the future, traveler_profile_id belongs to this user, etc.). If invalid, return 400 with error message.
Create a new record in auto_booking_requests (or campaigns) table with status = active (and possibly sub-status “search_pending”), and store all criteria. It will also set a created_at and updated_at. If an end_date or expiration is provided, store that.
Optionally, schedule the first search. We might do an immediate trigger by setting next_search_at = now() or even calling the search logic right within this request (though that could make the request slow; better to offload to the scheduler unless we want to do a quick test search synchronously).
Return 201 Created with the campaign data (including its generated ID and status). The response should exclude sensitive info but can include a summary like “Trip to X from Y, budget $Z, status active”.
2. GET /campaigns – List Campaigns:
Behavior: Fetch all campaigns for the authenticated user from the DB, possibly filtered by active/archived status. We will likely show active campaigns and maybe recently completed ones. This lets the front-end display the user’s campaign dashboard. Support query params like status=active or include=archived as needed.
Response: Array of campaign objects, including current status and maybe some metadata (e.g., last_searched_at, found_price if we store it, etc.). This should be efficient – ensure an index on user_id for quick retrieval. 3. GET /campaigns/:id – Get Campaign Details:
Behavior: Return the campaign if it belongs to the user. Could include more detailed info like the full criteria, and perhaps a history of attempts or last found price if we store those. This might not be needed if the list already provides enough, but could be useful if we have a detailed view. 4. PUT /campaigns/:id – Update Campaign:
This serves two purposes: (a) editing criteria, and (b) pausing/resuming. We can distinguish by the fields provided:
To pause: the front-end could call PUT with status: "paused" (or we provide a dedicated /campaigns/:id/pause endpoint for clarity). Similarly for resume (status: "active").
To edit criteria: allow certain fields like max_price, date range, etc., to be updated. We must ensure that lowering the budget or changing destination after creation is handled properly (likely we allow increasing budget or narrowing dates, but not completely changing the trip idea – that might effectively be a new campaign).
We will fetch the campaign, ensure it’s not already booked or cancelled, apply updates, and save. If the campaign was active and gets paused, we update status and ensure the scheduler will skip it. If resumed, set status active and maybe next_search_at to now to immediately rejoin the hunt.
If critical fields are changed (like max_price lowered), we might reset some internal tracking (for example, clear last found price if stored).
Respond with the updated campaign object.
5. DELETE /campaigns/:id – Cancel Campaign:
Behavior: Mark the campaign as cancelled (status = cancelled or we can actually delete the row, but better to soft-delete for record-keeping). If we soft-delete, we might use an is_active boolean or a separate status. We will also cancel any ongoing processing: in practice, the background job will skip anything not active, so just marking it is fine. If a booking is in progress at the exact time of cancel, we handle that carefully: ideally we prevent cancellation if a booking attempt is currently underway (or we treat that cancellation as “cancel after this attempt”). For simplicity, we might disallow deleting while booking_in_progress to avoid confusion.
Respond 204 No Content on success or appropriate error if, say, campaign already completed (we can still allow delete for completed – just removes it from list).
These endpoints correspond to the missing files noted, and implementing them will complete the campaign management backend. They rely heavily on the database schema, so let’s outline the key tables/fields involved: Database Schema:
auto_booking_requests (Campaigns): Fields: id (PK), user_id (the owner), traveler_profile_id, payment_method_id (references to a table storing Stripe payment info or just store the Stripe PM ID string), origin, destination, date_start, date_end (or perhaps trip_start and trip_end if range; could also store a CRON expression or frequency if it’s a recurring search concept), max_price, cabin_class, status, created_at, updated_at, last_searched_at, next_search_at, and maybe notes or found_offer_id (temp storage of an offer being processed). This table exists based on analysis and is likely named as such.
booking_requests and booking_attempts: Likely used by the state machine for idempotency and logging attempts. booking_attempts serves as a lock (with a unique constraint on campaign_id or similar to avoid duplicates). We’ll utilize these as is, ensuring our campaign_id ties in. We might add a foreign key from booking_requests to campaign_id if not already (to link an attempt to the campaign).
bookings: Stores finalized bookings (ticket info, etc.) with a foreign key to campaign (for auto bookings). If the system was originally built for manual bookings, we might need to extend it to mark which bookings came from auto campaigns (e.g., add campaign_id nullable field). This way we can show the user which booking resulted from which campaign.
traveler_profiles: (Proposed) to store traveler personal data. It may have fields as described (name, DOB, passport, user_id, etc.). The architecture suggests this table exists or is planned. We will integrate it by using traveler_profile_id in campaigns.
payment_methods or profile fields: The user’s Stripe customer ID and default payment method are stored either in the profile or a separate table of saved methods. We ensure the campaign references the correct payment method entry. This way, if the user updates their default card, we can decide whether campaigns use the new default or stick to the originally chosen method – for MVP, simplest is campaigns always use the latest default unless specified otherwise.
No major schema changes are needed beyond maybe adding campaign_id references as described and possibly a status field here or there. The current schema “all required tables exist” was noted, so we mostly build on it.
Error Handling & Notification Strategy
Robust error handling is crucial for an automated system that operates without user in the loop. Our strategy is to fail safely, notify the user clearly, and allow recovery whenever possible. Payment Failures: If the Stripe charge fails (card declined, insufficient funds, etc.), the booking state machine will catch that error. In this case:
The booking is aborted (we do not call Duffel).
The campaign is moved to a paused or payment_failed state. We choose to pause the campaign automatically to prevent repeated failed attempts on the same card. This pause indicates user action is needed.
The user is notified via email/push that the payment failed and no booking was made. The notification includes the reason if we have it (e.g., “Card was declined” or “Card requires 3D Secure authentication”). We prompt the user to update their payment method in Parker Flight.
In the app, the campaign will show an alert like “⚠️ Payment issue – action required”. The user can then go to their profile, add a new card or fix the issue, and then resume the campaign. We’ll provide a one-click “Retry booking” or “Resume campaign” once a valid payment method is on file. This ensures the user doesn’t lose the opportunity entirely – though by the time they fix it, that specific deal is likely gone, the campaign will continue watching for new deals.
Offer Expiration or Booking Failure: If Duffel returns an error during order creation (e.g., offer expired or no longer available):
If the error is offer_no_longer_available or similar, our code will interpret that as “deal missed.” We immediately initiate a refund of the user’s charge if it was already captured (in future we might authorize and capture only after successful order, but MVP captures then refunds on failure).
The campaign remains active by default (since no successful booking happened). We might implement a small cooldown to avoid immediate re-trigger if the same price is still out there. Essentially, it will try again on the next scheduled cycle.
Notify the user: This is a tricky case because we charged and then refunded. The user might see a charge pending on their card. We send a notification: “We found a flight for $X and attempted to book it, but unfortunately the seat was gone before confirmation. Any charge on your card has been voided or will not post. We’re still looking for new deals for you.” Transparency here is key to trust.
Log the event for internal monitoring. These should be rare if our timing is good, but we’ll track how often it happens.
Insufficient Duffel Balance: If using Duffel Balance and it’s insufficient, the Duffel API might return a payment required or balance error. Our booking function can check balance first to prevent this. If it still happens:
Treat similar to booking failure: refund user (or since we wouldn’t have charged if we knew, ideally we check beforehand).
Notify admin/ops immediately to top-up the balance, and possibly notify user that booking failed due to technical issues.
Possibly, if this scenario occurs, pause the campaign and have support reach out, because the user did everything right but Parker’s account had an issue. However, with proactive balance checks, we avoid this.
External API Failures (Search or Other): If the flight search step fails (Duffel or Amadeus returns error or times out):
The campaign remains active and will simply try again in the next cycle. We increment a retry counter and maybe implement an exponential backoff for that campaign (e.g. if Duffel is down for an hour, we don't spam every minute; we could detect a global outage and pause all for a bit).
If only one campaign fails due to a transient issue, it’s fine to try next hour.
No user notification on a single transient search failure (the user doesn’t need to know if one hourly check didn’t happen). But if an extended outage or repeated failures occur (say nothing could be checked for 24 hours), we may notify: “We’re experiencing technical difficulties checking flights for your campaign, please stand by.”
Implement monitoring/alerting for us: if, for example, 5 consecutive search attempts for a campaign fail or if the whole scheduler fails to run, raise an alert internally.
Double Booking Prevention: In case our locks or idempotency fail (shouldn’t happen), if a duplicate booking attempt is detected, one should back off:
The booking function will see the booking_attempts entry and just exit if already processing. That error doesn’t affect user, it just avoids a race. We log it.
If somehow two bookings succeeded for one campaign (very unlikely), we’d have to cancel one ticket manually. But with locks, this is practically mitigated.
Notification Strategy: We ensure users get timely notifications for:
Booking Success: When a booking is completed, trigger an email with the itinerary and confirmation. The existing notification integration covers this (it was in the state machine already). We may also send an in-app notification or SMS if critical.
Payment Failure: As described, notify with instructions to fix payment.
Deal Found but Failed to Book: Notify that a deal was found but could not be booked (expired/tech issue) – as a courtesy, we might include details like “Found NYC-LAX for $300, but it sold out before confirmation.” This shows the user value (we did find something) even though it didn’t go through, and reassures them we refunded and will keep trying.
Campaign Expiration: If a campaign reaches its end date or is about to expire with no success, send a notification like “Your campaign for X has ended – no flight under $Y was found. You can adjust criteria or start a new campaign.”
Campaign Paused (by system): If we auto-pause a campaign (payment failure or too many fails), notify the user that it’s paused and what they need to do (e.g. update card, then resume).
General Updates (optional): We might periodically notify long-running campaigns of progress, e.g. “Still looking for your trip to Paris. Hang tight!” – though this might be unnecessary noise. Instead, users can always check the app for status.
All notifications will be sent via our existing channels (likely using the notifications table and a service that sends emails). They will be phrased in a user-friendly way, avoiding technical jargon. E.g., instead of “PaymentIntent failed with authentication_required,” we say “Your bank needs additional verification. Please confirm your card or use a different card.” Error Logging & Admin Alerts: Internally, we implement logging for all critical failures. For instance:
If an auto-book attempt fails, log the campaign ID, reason, and stack trace if applicable.
Integrate with an error tracking service (Sentry or similar) for the Edge Functions, so exceptions are caught and reported.
Set up specific alerts: e.g., if any booking fails after payment success (meaning a refund was issued), flag that to the dev team because it might indicate timing issues. If many payment failures occur, maybe our card capture process can be improved.
Monitor third-party status (Stripe, Duffel). We might use their status webhooks or just handle errors as they come. For example, if Stripe is returning a lot of API errors, escalate to a “Stripe outage” mode and pause new bookings until resolved, to avoid thrashing.
Graceful Degradation: In extreme cases like one of our systems being down (Supabase outage, etc.), our approach is to ensure no inconsistent state:
If the scheduler fails to run (Supabase cron issues), campaigns simply won’t be processed until it’s fixed. Once back, they resume.
If a booking process crashes mid-way (e.g., after charging card but before creating order), our Stripe webhook and Duffel webhook will help recover:
Stripe’s webhook will show a successful payment with no booking recorded; we can catch that and attempt to complete the booking or refund automatically.
This is complex to automate fully, but at least we’ll be aware via logs and can manually intervene if needed.
In a worst-case scenario (system down when a deal appears), the deal might pass by. This is unfortunate but the user is no worse off than without our service. We might later communicate “We had downtime, sorry we might have missed some deals.”
Overall, the system errs on the side of protecting the user’s money and transparency. If something goes wrong, we don’t charge the user or we refund promptly, and we tell them what happened. By pausing campaigns on issues, we ensure no repeated errors without user awareness. This strategy aligns with building trust – users know the feature will either get them a flight or at least not charge them unexpectedly.
Performance and Scalability Considerations
As we deploy auto-booking, we must ensure it can scale to many users and high volumes of campaigns and searches without degrading performance or running into limits. Efficient Scheduling: Running a search for each campaign every hour could become heavy if we have, say, 10,000 campaigns. That might mean 10k API calls per hour to Duffel or Amadeus. A few measures to handle scale:
Batching searches: If multiple campaigns have similar routes or destinations, we could batch them into one API call. For instance, if 50 users all want flights from NYC to LAX, we don’t need 50 separate searches. We could do one search and then apply 50 different price thresholds to the results. This requires grouping campaigns by search criteria. It adds complexity, so it might be a phase 3 optimization. Initially, we isolate searches per campaign for simplicity.
Staggering schedule: Instead of all campaigns at 00 minute of the hour, we can stagger checks throughout the hour. E.g., use the campaign ID or creation time to assign it a minute within the hour to check. This smooths API usage and avoids spikes. Supabase cron can’t easily stagger per user, but our processor could internally distribute work (or we schedule the cron to run every few minutes and do a subset each time).
Rate limiting: We implement rate limiters for external APIs. Duffel’s published limits are something like 120 search requests per minute. We should enforce that in code (our DuffelService can queue or delay calls if needed, or use tokens). If the volume grows, we coordinate with Duffel for higher limits or use multiple accounts if absolutely needed (though Duffel might not allow that).
Query optimization: Ensure DB queries for campaigns are indexed (likely on status and next_search_at). This way, fetching due campaigns even out of thousands is fast. The auto_booking_requests table should have an index on (status, next_search_at) for that selection.
Parallel Processing: Using multiple Edge Function invocations in parallel (one per campaign as described) means we can scale horizontally. Supabase Edge Functions scale automatically with demand, but we must be mindful of limits (e.g., memory, execution time). If 100 campaigns trigger at once, that’s 100 concurrent function executions – this should be fine, but we should monitor. If needed, we can throttle how many campaigns we process simultaneously (e.g., process 50 then next 50) to avoid saturating the CPU or hitting Stripe API concurrency limits (Stripe can handle many, but if we attempt too many charges at the exact same time, a few might be rate-limited; Stripe’s limits are high though, and we use idempotency keys to be safe). Database Load: Each booking involves multiple DB writes (insert booking, update campaign, etc.). With thousands of campaigns, write load is not too high – even 10k campaigns, each checking maybe a few times a day, is on the order of tens of thousands of writes per day, which Postgres can handle easily. The heavier aspect is possibly storing search results if we do (but we probably won’t store every result, just the ones we book or maybe log some stats). Memory and Timeouts: Flight search results can be large (hundreds of offers). We should only fetch what we need. For example, if using Duffel’s offer request, we can specify filters or just retrieve the first X cheapest offers. We need only the cheapest that meets criteria, not all of them. Limiting response size will keep our functions fast. We also ensure each function execution stays within time limits (Edge Functions typically have a timeout like 5-10 seconds by default, or sometimes more for long jobs). A search + booking flow might take, say, 2-5 seconds (search ~1-2s, charge ~1s, order ~1s). That’s acceptable. If it ever risked timing out, we could break tasks (e.g., confirm payment via webhook rather than waiting synchronously), but our current plan keeps it synchronous for simplicity. Scalability of Stripe and Duffel: Both are external managed services built to scale globally. Stripe can handle large volume of payments – our usage (even thousands of charges per day) is nowhere near their limits. We just have to watch out for API rate limits (Stripe’s are also high, but if we do a lot of list or update calls we should cache or avoid unnecessary calls). Duffel’s API has known rate limits (search and order). Our design of one search per campaign could be heavy if number skyrockets. If needed, we might implement a more event-driven approach (e.g., integrate a price alert API or have campaigns subscribe to a flight price tracking service – those are advanced ideas outside this scope). Scaling Background Processing: If Supabase’s single cron job calling our function becomes a bottleneck (for instance, one function invocation trying to handle 10k campaigns might not complete within a minute), our parallel invocation strategy alleviates that. We could further distribute by splitting campaigns among multiple cron schedules (like one function for campaigns A-M, another for N-Z by some key). But that complicates things. It’s likely unnecessary until we grow a lot. Testing at Scale: We will test the system with a high number of simulated campaigns to see how it behaves. If any part is slow, we optimize. For example, if retrieving offers from Duffel for many campaigns is slow, maybe their API allows multi-city or date range searches to cut down calls. State Machine Efficiency: The booking state machine is 720 lines of battle-tested code, which likely means it’s doing a lot. But since it’s already production-ready, we assume it’s optimized for performance (e.g., not doing excessive calls synchronously that could be deferred). We should review if anything in it could slow down under load (like waiting for seat selection might not apply if we skip it, etc.). If needed, we can toggle off non-critical steps for auto-booking to speed it up (for instance, if it was sending too detailed notifications or doing redundant checks that made sense in interactive context but not needed here). Multi-Region and International Scaling: Parker Flight might expand internationally. Supabase Edge Functions run globally by default, which is good for latency. Duffel endpoints are likely in one region (maybe US or EU), but that’s fine. Stripe is global as well. We should ensure our architecture handles time zones – e.g., a user’s “date” is interpreted correctly (store dates in UTC in DB and consider local time only for user display). Cron scheduling is based on server time (likely UTC) so all campaigns are checked relative to that; if we wanted to check something at a specific local time (not a requirement we have now), we’d need to incorporate offsets. Cost Considerations: Each search and booking costs something (Duffel may charge small fees or usage of their API, and Stripe has transaction fees). While not an architecture issue, our design should allow throttling or limiting campaigns if needed. For example, if a user sets an extremely broad campaign that triggers hundreds of searches, that could incur cost. We might impose some limits (maybe allow up to X searches per campaign per day or encourage narrower criteria) to control cost. Or implement a small fee for the user if they run too many campaigns (depending on business model). These are product decisions, but architecture supports it by centralizing search logic (so we can add such checks easily). Security and Isolation: From a performance angle, we ensure that one user’s heavy campaign doesn’t break others. With each campaign mostly isolated, the worst case is one user has many campaigns – they’ll just consume more share of the background cycles. We could place a limit (like max 3 active campaigns per user in MVP) to avoid abuse, which again is a business rule. In summary, the system as designed should scale to the initial target (let’s say a few thousand users with maybe one campaign each) without issues. As it grows, we have a roadmap: introduce batching of searches, optimize cron scheduling, and possibly partition the processing. The reliance on serverless function concurrency and managed services means we can scale out rather than up – adding more parallelism if needed. We will also monitor performance metrics (duration of each function call, success/failure counts, etc.) to proactively spot bottlenecks.
Implementation Roadmap
Finally, we outline a phased implementation plan to build this architecture in a manageable way, leveraging the already completed 85% and layering on the missing 15%: Phase 1: Core Campaign Automation (MVP)
Goal: Get a basic end-to-end auto-booking working with one user, one campaign, on a live system. Focus on the happy path.
Campaign CRUD & Basic Monitoring: Implement POST /campaigns (creation with minimal validation), GET /campaigns (list user’s campaigns), and perhaps a simple DELETE. The front-end should be able to create a campaign with criteria and see it listed. The campaign processor cron runs say every hour and simply logs that it checked (we can initially even simulate finding a deal by a stub). This proves the wiring of scheduling. Then integrate a real flight search for a narrow case (e.g., fixed date, using Duffel test environment). Also implement the invocation of the booking function but initially perhaps point it to a stub or a dry-run mode of the state machine. Gradually, enable the real booking flow with Stripe test key and Duffel test API. Ensure a full cycle: create campaign -> (simulate a low price) -> charge test card -> create Duffel order in test -> record booking -> notify (perhaps just log or email to dev).
Duffel Order & Payment Integration: Complete the DuffelService functions needed (createOrder, etc.) and test with Duffel sandbox. Use Stripe test mode for payment. Ensure that the state machine can call these new Duffel functions instead of the placeholder. Because the existing state machine might currently use Amadeus for booking, we may need to refactor it: likely by abstracting the booking provider. We can create an interface like FlightBookingProvider with methods search/price/book, and implement one for Amadeus (existing) and one for Duffel. For auto-booking, we use the Duffel one. This way we don’t break any manual booking flows that used Amadeus. In MVP, if manual flow isn’t important, we can shortcut and directly call Duffel in our auto-book path.
Happy Path Testing: Use a test scenario (e.g., a known route with a test airline in Duffel sandbox) to run the entire flow. Validate that:
Campaign goes to booked state, booking record created.
Stripe shows a successful test charge.
Duffel shows an order created (in sandbox).
Emails (if integrated) are sent.
MVP Release: Possibly release to a small set of users or internal testers. The MVP will have limitations (no pause/resume via UI, minimal error handling, one traveler assumed, etc.) but should demonstrate the core value: automatically booking a flight deal within user’s budget.
Phase 2: Robustness and Feature Completion
Goal: Address critical missing pieces (error handling, pause/resume, traveler profiles) and harden the system for broader use.
Traveler Profile API: Implement the CRUD for traveler profiles (as mentioned in missing features) so users can save their info securely. Integrate this into campaign creation (so the user can pick which profile for the campaign). Ensure encryption of sensitive fields in the database at rest (Supabase vault or client-side encryption for things like passport if stored). Add input validation (correct date format for DOB, etc.).
Pause/Resume & Editing: Expose an endpoint or use PUT /campaigns to allow pausing a campaign. Also handle automatic pausing on payment failure as decided. The front-end should indicate paused status and let user resume (which calls the API). Implement editing of campaign budget or date if that’s allowed, with proper effects (e.g., if user lowers budget, ensure we don’t consider previous higher prices as acceptable).
Advanced Error Handling: Implement the notification and state changes for all error scenarios. This includes wiring up Stripe webhooks for payment_failed to pause campaigns, Duffel webhooks for ticketing (to finalize booking state), etc. By phase 2, the system should handle SCA-required errors gracefully: possibly by marking the campaign as requiring action and not attempting again until user intervenes (this is effectively paused with reason). If feasible, we might implement a way for the user to complete 3D Secure after the fact (Stripe provides a URL or hosting capability for that), but it might be complex – a simpler approach is just notify and pause.
Fallback and Recovery: Add logic for at least basic fallback: e.g., if Duffel search fails, call Amadeus as a backup to get price info. Or if a booking fails, notify as discussed. Also, incorporate any Duffel-specific webhook events to catch things like delayed ticketing (so we don’t mark booked until actually ticketed, or we update when ticketed).
Security Audit & Polishing: By end of Phase 2, sensitive data paths are implemented (traveler data, payments). Conduct a security review – ensure PII encryption is working, Stripe keys are secure, no leakage in logs (double-check we’re not logging card IDs or personal info inadvertently). Also test the pause/resume flows thoroughly: e.g., pause while in the middle of waiting for a deal – system should skip it properly; resume should pick it up next cycle.
Improve Monitoring & Admin Tools: Add admin visibility such as an internal dashboard of active campaigns and their statuses, so support can intervene if needed. Not user-facing, but helps manage at scale. Also set up metrics collection (maybe count searches per hour, bookings per day, etc., feeding to a dashboard) to measure performance.
Phase 3: Scale-Out and Enhancements
Goal: Extend functionality (multi-currency, multi-traveler, optimization) and ensure the system scales to a large user base with international support.
Multiple Payment Methods & Backup Cards: Allow users to store multiple cards and specify which card per campaign. Implement the fallback charge to secondary card if primary fails (with user opt-in). This involves expanding the PaymentService and UI to manage multiple payment options. Also handle card expiration notifications (Stripe can tell when a card is expiring – prompt user to update it ahead of time).
Multiple Travelers per User: Enable campaigns for different traveler profiles under the same account. E.g., a user can create a campaign for their spouse or child. This may mean one user (payer) covers multiple people. Our data model already supports linking a campaign to any traveler profile. We just need to ensure the booking uses the correct traveler info. Also adjust notifications to include traveler’s name (so user knows who it was for). If needed, allow storing travel documents like TSA Pre or loyalty numbers, though those are enhancements.
Global Currencies and Localization: If expanding beyond a single currency, integrate currency selection in campaign criteria (or infer from origin/destination). Ensure that the Stripe charge uses the correct currency. For example, if a user in Europe wants to pay in EUR, we create PaymentIntent in EUR. Duffel offers come in a specific currency (usually based on point of sale or airline). We might have to convert or specify currency in the search. Ideally, we let Duffel return prices in the desired currency (Duffel might let us specify a currency or it chooses automatically). Test end-to-end in a non-USD currency. Also, adjust email templates and UI to handle different currency symbols and formatting.
Optimizations for Scale: As user count grows, revisit the search strategy. Possibly implement grouping of searches or a smarter schedule as mentioned in Performance section. Also consider introducing event-driven triggers: e.g., if we integrate a price alert API or subscribe to Duffel’s upcoming feature (if any) for price change notifications. This could supplement the polling approach to catch deals faster and reduce unnecessary searches.
Machine Learning for Deals (future): In a later phase, Parker Flight might integrate a service that predicts or scrapes deal info (for example, monitoring public fare sales). That could feed into campaigns. While beyond our current scope, our architecture is flexible to allow external triggers – e.g., an external event could directly invoke a specific campaign’s booking function if it knows a deal meets criteria.
Continuous Hardening: As we scale, perform load tests. Maybe simulate 10k campaigns, see how the system behaves (in a staging environment to avoid hitting real APIs too much – we can mock Duffel/Stripe for this test). Use results to fine-tune DB indexing, function memory, etc. Also, engage in a formal security audit (especially before scaling internationally) to ensure compliance with GDPR, PCI (Stripe covers most, but our usage of PII should be reviewed).
User Feedback and Iteration: Gather feedback from users who have used auto-booking. Maybe they want to be able to tweak their campaigns on the fly, or see more info (like what prices have been found so far). Use this to guide further features: e.g., implementing that “progress update” notification or allowing a campaign to continue watching for a better deal even after one booking (like a subscription for repeated bookings – not initially planned, but possible).
Throughout these phases, we ensure that at each milestone, the system is deployable and adds value. Phase 1 delivers the core promise (even if only to beta users). Phase 2 makes it stable and trustworthy for broader release. Phase 3 makes it powerful and ready for large scale and international users. By following this roadmap, Warp’s engineering team can iteratively develop the feature, constantly verify it against real scenarios, and de-risk the launch of this ambitious auto-booking system.